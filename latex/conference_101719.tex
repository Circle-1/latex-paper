\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\begin{document}

\title{Analysis of Stock Market data and providing predictions using CNN-LSTM Neural Network model\\
}

\author{\IEEEauthorblockN{Aadhitya A}
\IEEEauthorblockA{\textit{Department of Information Technology} \\
\textit{Madras Institute of Technology}\\
Chennai, India \\
Email: aadhitya864@gmail.com}
\and
\IEEEauthorblockN{Rajapriya R}
\IEEEauthorblockA{\textit{Department of Information Technology} \\
\textit{Madras Institute of Technology}\\
Chennai, India \\
Email: N/A}
\and
\IEEEauthorblockN{Vineetha R S}
\IEEEauthorblockA{\textit{Department of Information Technology} \\
\textit{Madras Institute of Technology}\\
Chennai, India \\
Email: N/A}
\linebreakand
\IEEEauthorblockN{Anurag Manoj Bagde}
\IEEEauthorblockA{\textit{Department of Information Technology} \\
\textit{Madras Institute of Technology}\\
Chennai, India \\
Email: N/A}
}

\maketitle

\begin{abstract}
Stock market is often important as it represents the ownership claims on businesses. Without sufficient stocks, a company cannot perform well in finance. Predicting a stock market performance of a company is nearly hard because every time the prices of a company’s stock keeps changing and not constant. Therefore, it’s random to predict and pick the right company to buy or sell stocks. But if the previous performance of a company in stock market is known, then we can track the data and provide predictions (which is near accurate) to the stockholders to wisely take decisions on handling the stocks to a company. To handle this, many machine learning models have been invented but they didn't succeed due to many reasons like absence of advanced libraries, inaccuracy of model when made to train with real time data and much more. So, to track the patterns and the features of data, a CNN-LSTM Neural Network can be made. Recently, CNN is now used in Natural Language Processing (NLP) based applications, so by identifying the features from stock data and converting them into tensors, we can obtain the features and then send it to LSTM neural network to find the patterns and thereby predicting the stock market for given period of time. The accuracy of the CNN-LSTM NN model is found to be high even when allowed to train on real-time stock market data. This paper describes about the features of the CNN-LSTM model, experiments we made with the model like training with stock market datasets, performance comparison with other models and the end product we obtained at final stage.
\end{abstract}

\begin{IEEEkeywords}
CNN-LSTM, Deep Learning, Prediction, Pattern Recognition
\end{IEEEkeywords}

\section{Introduction}
Stock Market (also called as share market) is a place where people or businessmen often look on their "shares" or ownership claims related to the company. Stock market is considered an important place in economy as a country's wealth is decided on it. The hardest task in stock market is predicting it because the data is not constant, always. In early days, many people tried to predict stocks using conventional methods but most of the time, failed. So the probability of predicting stocks correctly is very minimal. Today, Machine Learning (ML) techniques have been implemented to predict the company shares and thereby providing suggestions to stock holders to improvise the company's performance. But however, they are not accurate. This paper focuses on some of the works done in predicting stock market and a new method to follow CNN-LSTM Neural Network model approach to predict data for given time-series.

\section{Related Work}

Before the time of writing this paper, many have proposed and implemented various algorithms in order to predict stock market data. We did a literature survey to find some of the algorithms proposed and found some of the advantages, disadvantages present in those algorithms. Subhadra and Kalyana in \cite{b1} conducted analysis in various machine learning methods to predict stock market data and found that Random Forest performs good compared with Linear Regression and other algorithms, however the error percentage rises in the model when the input data is not smoothed in pre-processing stage. Pang, Zhou et al in [2] made comparisons with RNN and LSTM model and conducted experimental analysis, in which LSTM with Auto-Encoder module enabled (AELSTM) predicted most of the data but the implementation is done using old libraries and with real-time stock market data, thus the accuracy of the model was low. This was improved but revealed an important point when a model is made to train with real-time data. Uma and Kotrappa in [3] proposed a new method where LSTM with Log Bilinear layer before it predicted most of the stock market data and turned out with high accuracy but it was proposed and not tested with real time data, also it was meant to predict data only during the time of COVID-19 and not beyond that. 

% use \cite for references! IMPORTANT!

[To be done by others]

\section{Proposed Work}
After we conducted the literature survey, we realised some of the key points to be taken while designing a ML model
\begin{itemize}
\item The model must be designed in such a way that it should parse real-time data and not just sample data
\item The data has to be preprocessed correctly in order to avoid errors during training and testing phase
\item The accuracy of the model not only depends upon it's parameters but the dataset as well
\item The only point most of the papers didn't mention is the deployment, as it's one of the most important fact while creating a model
\end{itemize}
Considering the above key points, we focused on making the ML model. We decided to go on with CNN-LSTM Neural Network (Convolutional Neural Network and Long Term Short Memory Neural Network) approach because CNN helps in tracking the features of dataset and LSTM helps in tracking the patterns, allowing to train on them. This approach isn't the first time as some already tried to implement it but we tweaked the parameters and layers to experiment and test it on real-time data. Since this is a regression type of problem where we had to train with time-series data, we used Mean Square Error as the standard metric rather than accuracy (because accuracy will return 0.0000+e00 even if loss value is present)

\textbf{NOTE: For this project, we mainly used Python and Jupyter Notebooks to perform the experiment. To see the experiment we did, refer \url{https://github.com/Circle-1/Stock-X}}

\section{Experimental Analysis}
For the experiment, we used Python for data preprocessing and creating a Neural Network model. For the application part, we used Flask framework using Python for backend and NextJS framework using JavaScript for frontend. For deployment, it's considered as minimal because, we saved the model in HDF5 format using Keras, but in addition to that, we made a Docker Image and Helm charts for people to work on. In this section, the major parts of the project and the experiments performed are described.

\subsection{Data Preprocessing and Analysis}\label{A}
In the process of creating a ML model, the first and foremost step is the data processing step; without that the model's accuracy won't be stable. So for this phase, we first cleansed the data by removing NULL values from the dataset and taking the mean of data and replacing it if necessary using Pandas library. Then we took the four columns of any stock market dataset, namely "Open", "Close", "High", "Low". These are the columns which mainly involve in training the dataset especially "Close" column (shown in Fig. \ref{dataset-1}). The graphs are plotted using the matplotlib library in Python. After making a primary analysis, we then went towards the preprocessing step.

Before we went on working towards preprocessing, we realised that CNN always considers 2-Dimensional arrays to train and select required features. But here, the data we have is of 1-Dimensional arrays. This is why CNN is always seen in Computer Vision (CV) based applications and not in NLP based applications. 

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.7]{Annotation-1.png}}
\caption{Example of stock market dataset}
\label{dataset-1}
\end{figure}

So, in order for CNN model to parse the dataset, we made a function where the 1-D arrays are made to convert to [100,1] tensor (precisely, a vector). Tensors are a type of data structures that describe a mulilinear relationship between set of objects in a vector space. So, for converting 1-D array to tensor, every 100 rows are taken and from that the mean of the values are calculated and made to store in a separate column. This process is done for entire dataset. In our case, we did this on the "Close" column as it's the main column where we would decide the prediction of the stock data.

After this step, we would obtain tensors for CNN side of model to train. Then, we split 80\% for training and 20\% for testing. Finally, we reshaped the data and sent it to the training phase. 

\subsection{Training Phase}\label{B}
After the dataset is processed, the NN model has to be made. In our case, it's the CNN-LSTM Neural Network model. 

For our model, we considered to divide the model into two parts, CNN and LSTM.

\subsubsection{CNN}
For the CNN section of model, we followed a custom way instead of ascending kind of way in the size of layers. So, we made 3 layers of neuron size 64,128,64 with MaxPooling layers in between. Finally we added a Flatten layer at end of CNN section to convert the tensors back to 1-D array.

All CNN layers are added with TimeDistributed function in order to train every temporal slice of input, as we're approaching a Time-Series problem in this case. Then, the processed data is sent to LSTM layers.

\subsubsection{LSTM}
For the LSTM section, we made 2 Bi-LSTM layers to detect the features and train them forward \& backward. For each layer, the neuron size is 200. Additionally, dropout layers are added in between with value of 0.25 and 0.5 in drop some features for stability.

Last, we added a dense layer with linear activation function and at the final layer we used "adam" optimizer ("sgd" also worked in this case but we found "adam" optimizer to be accurate after analysis), Mean Squared Error (mse) as the loss function and "mse" as the metric which is explained at first.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.5]{model_arch-1.png}}
\caption{The proposed model architecture}
\label{model}
\end{figure}

The architecture for the model is shown in Fig. \ref{model}. After the model is trained, we made it to plot the graph for the loss values (both training and validation) and at first it proved to be less and later varied accordingly (shown in Fig. \ref{loss-1}.). But the loss and mse varies when the model is saved and made to train again with saved parameters (this is fully explained in the testing phase). Then, we managed to refine the test dataset back to arrays using the reshape() function, then made the model to predict the dataset. The graph is plotted and the results proved to be great. The model was able to predict the stock data with given time series and it's shown in Fig. \ref{predict-1}. 

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.7]{loss_1.png}}
\caption{The loss graph obtained during training}
\label{loss-1}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.7]{predict_1.png}}
\caption{The prediction graph for sample NSE/NASDAQ data}
\label{predict-1}
\end{figure}

\subsection{Testing Phase}
After the model has been trained and the values are noted, we saved the model in HDF5 format using Keras API in TensorFlow library. Then, we loaded the HDF5 file and tried training the model again but this time with the different dataset, we were able to train the model but the loss value varies accordingly. For example, if the loss is 0.055 during training phase, the loss increases to 0.153 (estimated, not accurate). It is also found that this happens depending on the dataset we use, for NIFTY sample dataset the error didn't occur whereas in the NASDAQ dataset it occured while loading up the saved model.

We tested and experimented the model with different datasets to find how the model copes up with different stock markets like NYSE, NASDAQ, NIFTY and so on (shown in Fig. \ref{predict-1} and Fig. \ref{predict-2}). 

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.7]{predict_2.png}}
\caption{The prediction graph for sample NIFTY data}
\label{predict-2}
\end{figure}

\subsection{Deployment}
This section is considered as minimal one because we used Kaggle notebooks in order to create and deploy the models but we realised some developers who don't work with Kaggle and prefer local environments and tools like Conda. So, we made a Docker Image for them in order to work with Jupyter notebook with the GitHub repository as the folder. Initially, it was hard because by default, Docker Images are huge in size because of the OS images like Ubuntu. We got the image size as 250MB at first which is compressed and 750MB when uncompressed. We also tried to use pre-defined images for Jupyter notebooks where the libraries are already installed and ready, but we didn't use them as they are of huge size (approximately 3GB).


\section{End Product}
[To be done by others...]

\section{Comparison with other models}
After making and experimenting the model, we obtained the final product, which it can analyse and predict most of the stock market data. To check the performance, we did analysis with other models obtained from literature survey and got the below results.

% Table here...

\section {Conclusion}
[TBD]


\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}

\end{document}
